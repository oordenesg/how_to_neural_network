# -*- coding: utf-8 -*-
"""Neural_Network_Classification_Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fqzi9wdwqt76RA_JAV0HrQISkEgdRfXV

## Classification Exercises

### Introduction

In this exercise we will learn the architecture of a neural network, the parameters of a neural network and how they change with changing architecture and the hyperparameter tuning. 

We encourage you to work with other hyperparameters as well like learning rate, number of layers, activation functions etc. 

The Notebook is divided in three parts: Building the Model, Reading the dataset and Hyperparameters. It contains five sesctions in total and one additional optional exercise:

### Part 1: Building the model
Below we define a function to built a neural network model using TensorFlow Keras.
"""

import tensorflow as tf
import numpy as np
from tensorflow import keras

def built_model(input_shape, n_hidden, nb_classes, optimizer='SGD'):
  '''
  input_shape: The number of inputs to the neural network
  n_hidden: Number of hidden neurons in the hidden layers
  nb_classes: Number of neurons in the output layer
  optimizer: The optimizer used to train the model. 
  By default we use Stochastic Gradient Descent.
  
  Returns:
  The function returns A model with loss and optimizer defined
  '''  
  model = tf.keras.models.Sequential()
  ## First Hidden layer  
  model.add(keras.layers.Dense(n_hidden,
       input_shape=(input_shape,),
       name='dense_layer', activation='relu'))
    
  ## Second Hidden Layer
  model.add(keras.layers.Dense(n_hidden,
        name='dense_layer_2', activation='relu'))
    
  ## Output Layer  
  model.add(keras.layers.Dense(nb_classes,
        name='dense_layer_3', activation='softmax'))
    
  ## Define loss and optimizer 
  model.compile(optimizer=optimizer, 
              loss='categorical_crossentropy',
              metrics=['accuracy'])
  return model

# Defining the structure of our neural network
INPUT_SHAPE = 5
N_HIDDEN = 10
NB_CLASSES = 2

model = built_model(INPUT_SHAPE, N_HIDDEN,NB_CLASSES)

print(model.summary())

"""<a id='ex_2'></a>
**Here, it is important to note that our neural network has 192 parameters. As the number of networks increases, so does the complexity and execution time. Now, let's visualize the summary of the model created**
"""

model.summary()

"""### Part 2: Reading the dataset

We will continue with the MNIST dataset. 

###### Just run the cells in this part of the notebook. Do not change anything.
"""

mnist = keras.datasets.mnist
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Processing the data
assert(len(X_train.shape)==3), "The input data is not of the right shape"
RESHAPED = X_train.shape[1]*X_train.shape[2]

X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# Data Normalization
X_train, X_test = X_train / 255.0, X_test / 255.0
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

"""For the MNIST dataset the number of input and number of output units are fixed. However we can choose different values of hidden units. """

INPUT_SHAPE = RESHAPED
NB_CLASSES = len(set(Y_train))

# one-hot encode
Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES) 
Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)

"""### Part 3: Hyperparameters

<a id='ex_3'></a>
The aim of this section is to understand the affect of changing number of hidden units on the model performance. Change the number of hidden units, and train the model. Compare the model performance in terms of accuracy. What do you understand from this?
"""

# Task to do choose different values for number of hidden units (minimum five different values)
N_HIDDEN = 10 #Choose a value

## Do not change anything below
model = built_model(INPUT_SHAPE,N_HIDDEN, NB_CLASSES)
history = model.fit(X_train, Y_train,
		batch_size=128, epochs=50,
		verbose=1, validation_split=0.2)

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, Y_test)
print('Test accuracy: {:.2f} %'.format(test_acc*100))

N_HIDDEN = [10,20,30,40,50]

## Do not change anything below
for i in range(0,len(N_HIDDEN)):
  model = built_model(INPUT_SHAPE,N_HIDDEN[i], NB_CLASSES)
  history = model.fit(X_train, Y_train,
		  batch_size=128, epochs=50,
		  verbose=0, validation_split=0.2)
  # Evaluate the model
  test_loss, test_acc = model.evaluate(X_test, Y_test)
  print(test_acc)

"""<a id='ex_4'></a>
### Part 4: Hyperparameters

Let us now repeat the same after changing the batch size (minimum 5 different values). Compare the model performance in terms of accuracy. What do you understand from this?

**Answer** 

10: 91.94%
20: 94.30%
30: 95.23%
40: 95.63%
50: 91.94%

The results show that as we increase the number of neurons, we will obtain better results. However, it does not mean that we have increased the number of neuons in the hidden layers abruptly as it could cause an overfitting.
"""

# Task to do choose different values for batch size (minimum five different values)
BATCH_SIZE = [8,16,32,64,128] # Choose a value

## Do not change anything below
model = built_model(INPUT_SHAPE,128, NB_CLASSES)
history = model.fit(X_train, Y_train,
		batch_size=BATCH_SIZE, epochs=50,
		verbose=1, validation_split=0.2)
# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, Y_test)
print('Test accuracy: {:.2f} %'.format(test_acc*100))

for i in range(0,len(BATCH_SIZE)):
  model = built_model(INPUT_SHAPE,128, NB_CLASSES)
  history = model.fit(X_train, Y_train,
		  batch_size=BATCH_SIZE[i], epochs=50,
		  verbose=0, validation_split=0.2)
  # Evaluate the model
  test_loss, test_acc = model.evaluate(X_test, Y_test)
  print(test_acc)

"""There were no big differences using different values ​​in the BATCH_SIZE parameter. As this parameter increased, the accuracy decreased slightly.

<a id='ex_5'></a>
### Part 5: Hyperparameters

**Exercise 5:** And now we do the same with different [optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) available in TensorFlow. Change the optimizers and compare the model performance in terms of accuracy. What do you understand from this?
"""

# Task to do choose different optimizers
opt = ['sgd','adam']# Choose from available optimizers

## Do not change anything below
for i in range(0,len(opt)):
  model = built_model(INPUT_SHAPE,128, NB_CLASSES, opt[i])
  history = model.fit(X_train, Y_train,
		  batch_size=128, epochs=50,
		  verbose=0, validation_split=0.2)
  # Evaluate the model
  test_loss, test_acc = model.evaluate(X_test, Y_test)
  print(test_acc)

"""<a id='ex_O'></a>
### Optional Exercise: Fashion MNIST

Let's use a different input. You can use Fashion MNIST another popular ML dataset. Are the results same?.

To download fashion mnist you can use the following code:

```
fashion_mnist = keras.datasets.fashion_mnist

(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()
```
"""

fashion_mnist = keras.datasets.fashion_mnist
 
(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()

# Processing the data
assert(len(X_train.shape)==3), "The input data is not of the right shape"
RESHAPED = X_train.shape[1]*X_train.shape[2]

X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

# Data Normalization
X_train, X_test = X_train / 255.0, X_test / 255.0
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

INPUT_SHAPE = RESHAPED
NB_CLASSES = len(set(Y_train))

# one-hot encode
Y_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)
Y_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)

N_HIDDEN = [10,20,30,40,50]

## Do not change anything below
for i in range(0,len(N_HIDDEN)):
  model = built_model(INPUT_SHAPE,N_HIDDEN[i], NB_CLASSES)
  history = model.fit(X_train, Y_train,
		  batch_size=128, epochs=50,
		  verbose=0, validation_split=0.2)
  # Evaluate the model
  test_loss, test_acc = model.evaluate(X_test, Y_test)
  print(test_acc)

"""When comparing different numbers of neurons for this new data set, it is seen that the accuracy decreased compared to the results obtained with the previous data set."""